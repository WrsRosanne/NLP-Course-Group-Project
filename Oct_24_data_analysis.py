# -*- coding: utf-8 -*-
"""2025.10.24 Data Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FmQQ2m55PsgTXPmIHkl5JXQR7afyjSPY
"""

# bringing in some word processing power
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

# loading in the dataset
from google.colab import drive
drive.mount('/content/drive/', force_remount = True)

import os

file_path_owner = '/content/drive/MyDrive/all_songs_data.csv'
file_path_shared = r'/content/drive/MyDrive/QMSS5067 NLP Group Project/all_songs_data.csv'
#Before running this, add a shortcut to the shared group folder to My Drive

if os.path.exists(file_path_owner):
    df = pd.read_csv(file_path_owner)
    print("Read from owner's drive.")
elif os.path.exists(file_path_shared):
    df = pd.read_csv(file_path_shared)
    print("Read from shared drive.")
else:
    print("File not found in either location.")

# visualizing columns
df.head(1)

# Removing unecessary columns for our analysis
df_subset = df[['Song Title', 'Artist', 'Lyrics', 'Year']].copy()

# Removing NaN values for our Year and Lyrics and rounding year to int values for analysis
df_subset = df_subset[df_subset['Year'].notna()]
df_subset = df_subset[df_subset['Lyrics'].notna()]
df_subset['Year'] = df_subset['Year'].round().astype(int)

# Removing stopwords
def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return ' '.join(filtered_words)

df_subset['Lyrics_cleaned'] = df_subset['Lyrics'].apply(remove_stopwords)
df_subset.head()

# Removing brackets (this code is optional, as it may actually mess with the string more than desired)
def remove_brackets(text):
    if pd.isna(text) or not isinstance(text, str):
        return text

    # Remove brackets and everything inside them
    text = re.sub(r'\[.*?\]', '', text)  # Remove [...]
    text = re.sub(r'\(.*?\)', '', text)  # Remove (...)
    text = re.sub(r'\{.*?\}', '', text)  # Remove {...}
    return text.strip()

df_subset['Lyrics_cleaned'] = df_subset['Lyrics_cleaned'].apply(remove_brackets)

df_subset.head()

#Stemming lyrics
#use prof utils function
def stem_fun(str_i, sw_i):
    from nltk.stem import PorterStemmer, WordNetLemmatizer
    if sw_i == "porter":
        ps = PorterStemmer()
    else:
        ps = WordNetLemmatizer()
    t_x = list()
    for word in str_i.split():
        if sw_i == "porter":
            t_x.append(ps.stem(word))
        else:
            t_x.append(ps.lemmatize(word))
    text_x = ' '.join(t_x)
    return text_x

df_subset["stemmed_lyrics"] = df_subset["Lyrics_cleaned"].apply(lambda x: stem_fun(x, "porter"))
df_subset.head()

#Lemmatise lyrics
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
df_subset["lemma_lyrics"] = df_subset["Lyrics_cleaned"].apply(lambda x: stem_fun(x, "lemma"))
df_subset.head()

#count how much unique token after stemming and lemma
def cnt_tok(str_in, sw_in):
    if sw_in == "list":
        t = len(str_in.split())
    else:
        t = len(set(str_in.split()))
    return t

df_subset["stem_cnt"] =df_subset["stemmed_lyrics"].apply(
    lambda x: (cnt_tok(x,"set")))
df_subset["lemma_cnt"] =df_subset["lemma_lyrics"].apply(
    lambda x: (cnt_tok(x,"set")))
s = df_subset[["stem_cnt","lemma_cnt"]].describe()
print (s)

#pickle functions
def read_pickle(path_in, name_in):
    import pickle
    the_data_t = pickle.load(open(path_in + name_in + ".pk", "rb"))
    return the_data_t

def write_pickle(obj_in, path_in, name_in):
    import pickle
    pickle.dump(obj_in, open(path_in + name_in + ".pk", "wb"))

#define function for vectorisation

def x_formfun(df_in, col_n, m_in, n_in, label_in, o_path, name_i):
    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
    import pandas as pd
    if name_i == "TF":
        cv = CountVectorizer(ngram_range = (m_in, n_in))

    if name_i == "TF-IDF":
        cv =  TfidfVectorizer(ngram_range = ((m_in,n_in)))

    x_data_i =  pd.DataFrame(
            cv.fit_transform(df_in[col_n]).toarray())
    x_data_i.columns = cv.get_feature_names_out()
    x_data_i.index = df_in[label_in]
    write_pickle(cv, o_path, name_i)
    return x_data_i

#Perform transformation and vectorisation. Let's do TF-IDF
#unigrams and bigrams
x_formfun(df_subset, "Lyrics_cleaned", 1,1, "Year", file_path_shared, "TF-IDF")